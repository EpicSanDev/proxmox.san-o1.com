# San-O1 AI Infrastructure Deployment Configuration
# This file configures the deployment of AI infrastructure on Proxmox

# Proxmox API connection settings
proxmox:
  host: "192.168.0.168"  # Proxmox host address (port will be handled automatically)
  user: "root@pam"             # Proxmox user
  password: "Zabou007**Jule"    # Proxmox password
  verify_ssl: false            # Whether to verify SSL certificate

# Node selection configuration
node_selection:
  # Weight factors for node selection (must sum to 1.0)
  node_weights:
    cpu: 0.3
    memory: 0.3
    disk: 0.2
    network: 0.1
    gpu: 0.1
  
  # Storage preferences (prioritizes specific storages)
  storage_preferences:
    - ["SSD", 1.0]   # [storage_name, weight]
    - ["local", 0.7]
    - ["nfs", 0.5]
  
  # Minimum free resources required on a node (percentage)
  min_free:
    cpu: 10
    memory: 15
    disk: 20
  
  # Service resource requirements
  service_requirements:
    qdrant:
      memory: 8192      # MB
      cpu: 4            # cores
      disk: 50          # GB
    
    ollama:
      memory: 32768     # MB (32GB for large models)
      cpu: 8            # cores
      disk: 100         # GB
      gpu: "nvidia"     # Requires NVIDIA GPU
    
    n8n:
      memory: 2048      # MB
      cpu: 2            # cores
      disk: 20          # GB
    
    redis:
      memory: 4096      # MB
      cpu: 2            # cores
      disk: 10          # GB
    
    postgres:
      memory: 4096      # MB
      cpu: 2            # cores
      disk: 20          # GB
  
  # Services that should run on the same node (affinity)
  affinity:
    - ["ollama", "qdrant"]     # Keep vector DB close to LLM
  
  # Services that should run on different nodes (anti-affinity)
  anti_affinity:
    - ["redis", "postgres"]    # Separate data stores

# Load balancing configuration
load_balancing:
  deploy_lb: true             # Whether to deploy a load balancer
  node: null                  # Optional specific node for LB (null for auto-select)
  ha_enabled: false           # High availability option
  ssl_enabled: false          # SSL termination
  storage: "SSD"      # Storage for LB container
  cores: 2                    # Cores for LB
  memory: 2048                # Memory for LB (MB)
  swap: 1024                  # Swap for LB (MB)
  disk_size: 10               # Disk size (GB)
  base_vmid: 2000             # Starting VMID for LBs
  domain: "ai-cluster.local"  # Domain for service URLs
  network:
    bridge: "vmbr0"           # Network bridge

# Service deployment configurations
services:
  # Templates for container OS images
  templates:
    default:
      ostemplate: "local:vztmpl/ubuntu-22.04-standard_22.04-1_amd64.tar.zst"
    
    # Service-specific templates (optional)
    # ollama:
    #   ostemplate: "local:vztmpl/ubuntu-22.04-standard_22.04-1_amd64.tar.zst"
  
  # Default storage for all services
  default_storage: "SSD"
  
  # Starting VMID for services
  base_vmid: 1000
  
  # Network configuration for containers
  network:
    bridge: "vmbr0"
  
  # Qdrant vector database configuration
  qdrant:
    cores: 4
    memory: 8192
    swap: 2048
    disk_size: 50
    storage: "SSD"  # Storage for this service (overrides default)
  
  # Ollama AI configuration with NVIDIA GPU support
  ollama:
    cores: 8
    memory: 32768  # 32GB for large models
    swap: 8192
    disk_size: 100
    storage: "SSD"
    models:
      - "deepseek:32b"  # Will be pulled on deployment
  
  # n8n workflow automation configuration
  n8n:
    cores: 2
    memory: 2048
    swap: 1024
    disk_size: 20
    storage: "SSD"
  
  # Redis in-memory database configuration
  redis:
    cores: 2
    memory: 4096
    swap: 1024
    disk_size: 10
    storage: "SSD"
  
  # PostgreSQL database configuration
  postgres:
    cores: 2
    memory: 4096
    swap: 1024
    disk_size: 20
    storage: "SSD"
